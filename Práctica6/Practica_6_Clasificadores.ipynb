{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pr\u00e1ctica 6: Clasificadores de Distancia y Bayesianos\n",
    "**Alumno:** [Tu Nombre Aqu\u00ed]  \n",
    "**Materia:** Inteligencia Artificial / Pattern Recognition  \n",
    "\n",
    "Este notebook implementa:\n",
    "1. **Pre-procesamiento** (Escalado) del dataset Iris.\n",
    "2. Modelos **1NN, KNN (K=3,5,7,9) y Naive Bayes**.\n",
    "3. Validaci\u00f3n mediante **Hold-Out (70/30)**, **10-Fold CV** y **Leave-One-Out**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importaci\u00f3n de librer\u00edas y Carga de Datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Configuraci\u00f3n visual\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Cargar Dataset Iris\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Mostrar estructura b\u00e1sica\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "print(\"Dataset cargado correctamente.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Pre-procesamiento\n",
    "# Es CRUCIAL escalar los datos para algoritmos de distancia como KNN\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Datos escalados (media=0, var=1). Primeras 5 filas:\")\n",
    "print(X_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuraci\u00f3n de Modelos y Validaci\u00f3n\n",
    "\n",
    "modelos = {\n",
    "    \"1NN\": KNeighborsClassifier(n_neighbors=1),\n",
    "    \"KNN (K=3)\": KNeighborsClassifier(n_neighbors=3),\n",
    "    \"KNN (K=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"KNN (K=7)\": KNeighborsClassifier(n_neighbors=7),\n",
    "    \"KNN (K=9)\": KNeighborsClassifier(n_neighbors=9),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "resultados_lista = []\n",
    "\n",
    "print(\"Iniciando evaluaci\u00f3n de modelos...\\n\")\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    # A. Hold-Out 70/30\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    acc_holdout = accuracy_score(y_test, modelo.predict(X_test))\n",
    "    \n",
    "    # B. 10-Fold Cross-Validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    acc_10fold = cross_val_score(modelo, X_scaled, y, cv=kf).mean()\n",
    "    \n",
    "    # C. Leave-One-Out\n",
    "    loo = LeaveOneOut()\n",
    "    acc_loo = cross_val_score(modelo, X_scaled, y, cv=loo).mean()\n",
    "    \n",
    "    resultados_lista.append({\n",
    "        \"Modelo\": nombre,\n",
    "        \"Hold-Out (70/30)\": acc_holdout,\n",
    "        \"10-Fold CV\": acc_10fold,\n",
    "        \"Leave-One-Out\": acc_loo\n",
    "    })\n",
    "\n",
    "print(\"\u00a1Evaluaci\u00f3n finalizada!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Reporte de Resultados\n",
    "df_resultados = pd.DataFrame(resultados_lista)\n",
    "df_resultados.set_index(\"Modelo\", inplace=True)\n",
    "\n",
    "# Mostrar Tabla\n",
    "display(df_resultados.style.background_gradient(cmap='Blues', axis=None).format(\"{:.4f}\"))\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_resultados.plot(kind='bar', figsize=(12, 6), rot=0)\n",
    "plt.title(\"Accuracy por Modelo y M\u00e9todo de Validaci\u00f3n\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "1. **Escalado:** Fue necesario utilizar `StandardScaler` para que las distancias euclidianas en KNN no se vieran sesgadas.\n",
    "2. **Comparativa KNN:** Se observa que valores muy bajos de K (como 1) tienden a sobreajustarse, mientras que K intermedios (3-5) suelen dar mejor generalizaci\u00f3n en este dataset.\n",
    "3. **Validaci\u00f3n:** El m\u00e9todo `Leave-One-Out` ofrece la estimaci\u00f3n m\u00e1s robusta al usar casi todos los datos para entrenar en cada iteraci\u00f3n, aunque es computacionalmente m\u00e1s costoso que el `Hold-Out`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}